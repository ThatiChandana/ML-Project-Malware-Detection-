import numpy as np #mathematical calc
import pandas as pd #data manipulation
import seaborn as sns #graphical representation
import matplotlib.pyplot as plt #visualization
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

#loading the dataset
data = pd.read_csv('/content/uci_malware_detection.csv')
data.head()

from sklearn.preprocessing import StandardScaler  # to standardize the features

# DataFrame is used for cleaning and transforming data
df = pd.DataFrame(data)
data = df
data.head()

# Data Splitting
X = data.drop(["Label"],axis=1) #independent variables
y = data['Label'].values #target
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

# Reducing the columns with PCA
from sklearn.decomposition import PCA # importing the PCA

# apply PCA on independent variables
pca = PCA(n_components=6)
X_pca = pca.fit_transform(X) # used for consistency
# calc mean and std deviation and then apply standardization

df = pd.DataFrame(data=X_pca, columns=['F1', 'F2', 'F3', 'F4', 'F5', 'F6'])
# concatenate PCA transformed data with target variable
df['Label'] = y

# print first 5 rows of the new dataset
print(df.head())

data = df
data
X = data.drop(["Label"],axis=1)
y = data['Label'].values
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

# Data Processing
data.isnull().sum()
sns.countplot(x='Label', data=data);

#Training the data and finding accuracy using classifiers
# 1. Logistic Regression
lr_model =  LogisticRegression(max_iter=500 ,random_state=42)

# 2. Decision Tree
dt_model =  DecisionTreeClassifier()

# 3. Naive Bayes
nb_model = GaussianNB()

# 4. Random Forest
rf_model = RandomForestClassifier( n_jobs=-1)

# 5. KNN
knn_model = KNeighborsClassifier(n_jobs=-1)

# 6. AdaBoost
ada_boost_model = AdaBoostClassifier()

# 7. ExtraTrees
extra_trees_model = ExtraTreesClassifier()


######################### LIST OF ALL MODELS #############################3
ensemble_clf=[lr_model, dt_model, nb_model, rf_model, knn_model, ada_boost_model, extra_trees_model]

#print(ensemble_clf)
print(len(ensemble_clf))

# Classification Report

ensemble_clf=[lr_model, dt_model, nb_model, rf_model, knn_model, ada_boost_model, extra_trees_model]
models = [LogisticRegression , DecisionTreeClassifier, GaussianNB, RandomForestClassifier, KNeighborsClassifier, AdaBoostClassifier, ExtraTreesClassifier]


for i in range(len(ensemble_clf)):
    # Initialising model
    print('############################# '+str(type(ensemble_clf[i]).__name__)+' #############################')

    #Fitting on data
    ensemble_clf[i].fit(X_train, y_train)
    # to find the best parameters that fit the training data

    #Scoring the model on train data
    print("\n")
    print("Training Accuracy :\t ", ensemble_clf[i].score(X_train, y_train))

    #Scoring the model on test_data
    print("Testing Accuracy :\t  ",  ensemble_clf[i].score(X_test, y_test))


    y_pred = ensemble_clf[i].predict(X_test)
    print("Accuracy score : \t  ", accuracy_score(y_test, y_pred))
    print("\n")

    print('\033[01m              Classification_report \033[0m')
    print(classification_report(y_test, y_pred))
    print("\n")

    cf_matrix = confusion_matrix(y_test, y_pred)
    print('Confusion matrix:\n ', cf_matrix)
    plot_ = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,fmt= '0.2%')  # percentage of predictions that are correct
    plt.show()
    print("\n")

    model = []
    accuracy_test=[]
    for m in models:
        model_ = m()
        model_.fit(X_train, y_train)
        model_name = type(m()).__name__
        model.append(model_name)
        pred = model_.predict(X_test)
        acc = accuracy_score(pred, y_test)
        accuracy_test.append(acc)
# Plot graph for malicious
precision_score = []
recall_score = []

for i in range(len(ensemble_clf)):
    ensemble_clf[i].fit(X_train, y_train)
    y_pred = ensemble_clf[i].predict(X_test)

    cf_matrix = confusion_matrix(y_test, y_pred)
    precision = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])  #TruePositives / (TruePositives + FalsePositives)
    precision_score.append(precision)
    recall = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[1][0])  #TruePositives / (TruePositives + FalseNegatives)
    recall_score.append(recall)
bar_width = 0.20

# Set the x-axis locations
x_pos = np.arange(len(models))

# Create the figure and axis objects
fig, ax = plt.subplots(figsize=(15, 7))

# Plot the bars
rects1 = ax.bar(x_pos, precision_score, width=bar_width, color='blue', alpha=0.5, label='Precision')
rects2 = ax.bar(x_pos + bar_width, recall_score, width=bar_width, color='green', alpha=0.5, label='Recall')
rects3 = ax.bar(x_pos + 2*bar_width, accuracy_test, width=bar_width, color='red', alpha=0.5, label='Accuracy')

# Set the axis limits and labels
ax.set_ylim([0, 1])
ax.set_ylabel('Scores')
ax.set_xticks(x_pos + bar_width, model)
ax.set_xticklabels(model)

# Add a legend
ax.legend()

# Add values above the bars
def add_values(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 2), textcoords='offset points', ha='center', va='bottom')

add_values(rects1)
add_values(rects2)
add_values(rects3)

# Show the plot
print("PLOT GRAPH FOR MALICIOUS")
plt.show()

# Accuracy of all models
model = pd.Series(model, name='Model').astype(str)  # adds features like indexing
accuracy = pd.Series(accuracy_test, name='Accuracy')
output = pd.concat([model, accuracy], axis=1)
output

# Data Visualization of accuracy blw the models
plt.figure(figsize=(12, 7))
plots = sns.barplot(x='Model', y='Accuracy', data=output)
for bar in plots.patches:
    plots.annotate(format(bar.get_height(), '.2f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=15, xytext=(0, 8),
                   textcoords='offset points')

plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.xticks(rotation=20);

# Assigning the target values to Binary values
from sklearn.preprocessing import LabelBinarizer
lb_t = LabelBinarizer()
data['Label'] = lb_t.fit_transform(df['Label'])
data['Label'].unique()
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Correlation Blw the features without PCA
'''scalar = StandardScaler()
scaled_data = pd.DataFrame(scalar.fit_transform(X)) #scaling the data
sns.heatmap(data.corr())'''

# Best classifiers
# RandomForest Classifier
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)
classifier.fit(X_train, y_train)
#predict the test results
y_pred1 = classifier.predict(X_test)
#Making the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred1)
plot_ = sns.heatmap(cm/np.sum(cm), annot=True,fmt= '0.2%')  # percentage of predictions that are correct
plt.show()

# check for overfitting
from sklearn.model_selection import cross_val_score

scores = cross_val_score(classifier, X_test, y_test, cv=5)

# calculate the mean and standard deviation of the scores
mean_score = scores.mean()
std_score = scores.std()

# check if the model is overfitting
if mean_score - 2 * std_score > 0.9:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

# ExtraTrees Classifier
from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier(n_estimators=100, random_state=0)
etc.fit(X_train, y_train)
#predict the test results
y_pred2 = etc.predict(X_test)
#Makeing the confusion matrix
from sklearn.metrics import confusion_matrix
cme = confusion_matrix(y_test, y_pred2)
plot_ = sns.heatmap(cme/np.sum(cme), annot=True,fmt= '0.2%')  # percentage of predictions that are correct
plt.show()
# check for overfitting
from sklearn.model_selection import cross_val_score

scores = cross_val_score(etc, X_test, y_test, cv=5)

# calculate the mean and standard deviation of the scores
mean_score = scores.mean()
std_score = scores.std()

# check if the model is overfitting
if mean_score - 2 * std_score > 0.9:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

# GaussianNB classifier
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_test, y_test)
#predict the test results
y_pred3 = clf.predict(X_test)
from sklearn.metrics import confusion_matrix
cmg = confusion_matrix(y_test, y_pred)
plot_ = sns.heatmap(cmg/np.sum(cmg), annot=True,fmt= '0.2%')  # percentage of predictions that are correct
plt.show()
# check for overfitting
from sklearn.model_selection import cross_val_score

scores = cross_val_score(clf, X_test, y_test, cv=5)

# calculate the mean and standard deviation of the scores
mean_score = scores.mean()
std_score = scores.std()

# check if the model is overfitting
if mean_score - 2 * std_score > 0.9:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

# Plot the graph between the Acutual and Predicted output
'''%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn
plt.figure(figsize = (10, 7))
sn.heatmap(cm, annot = True)
plt.xlabel('Predicted')
plt.ylabel('Actual')'''
data.tail(20)
#mal-0
k = RandomForestClassifier()
k.fit(X, y)
import joblib
joblib.dump(k, 'malware')
model1 = joblib.load('malware')
pred = model1.predict([[-19.86757,	-654652.98765,	3.987654,	-1.987654,	-1,	-1]])
print(pred)
!pip install gradio
#Gradio is a Python library used for quickly creating customizable web interfaces for machine learning models.

import gradio as gr
import joblib

# Load the trained model
mod = joblib.load("malware")

# Define the prediction function
def Malware_exec_Detection(F1,F2,F3,F4,F5,F6):
    pred = mod.predict([[F1,F2,F3,F4,F5,F6]])
    return pred

# Create the Gradio interface
inputs = [
    gr.inputs.Number(label="Feature 1"),
    gr.inputs.Number(label="Feature 2"),
    gr.inputs.Number(label="Feature 3"),
    gr.inputs.Number(label="Feature 4"),
    gr.inputs.Number(label="Feature 5"),
    gr.inputs.Number(label="Feature 6")

]
output = gr.outputs.Textbox(label="Malware prediction")

interface = gr.Interface(Malware_exec_Detection, inputs, output, title="Malware Executable Detection")
interface.launch(share = 'True')











